# Franka Lift Cube 机械臂抓取任务文档

## 概述

本文档详细描述了基于 Franka Emika Panda 机械臂的物体抓取与提升任务环境。该环境是 MotrixLab 项目的一部分，提供了使用强化学习训练机械臂执行物体抓取、提升并移动到指定目标区域的完整实现。

|  |  |
| --- | --- |
| 动作空间 | Box(-inf, inf, (8,), float32) |
| 观测空间 | Box(-inf, inf, (36,), float32) |
| 导入方式 | `registry.make("franka_lift_cube", backend="np")` |

-----

## 环境描述

Franka Lift Cube 任务环境基于 Franka Emika Panda 七轴机械臂构建，旨在训练机械臂在桌面环境中抓取立方体（Cube），将其提升并移动到三维空间中的指定目标坐标。该环境使用 MotrixSim 物理引擎进行仿真。

### 环境组成

#### 机械臂
  Franka Panda 是一个高精度的轻量级机械臂，由以下主要部分组成：

  - **机械臂本体**：包含7个旋转关节（Joint 1-7）
  - **末端执行器（Gripper）**：包含两个指关节（finger\_joint1, finger\_joint2），但在动作空间中被简化为一个控制维度。
  
#### 目标物体
  一个可被抓取的立方体（Cube）,尺寸为 $0.02*0.02*0.02m$

#### 工作台
  用于放置机械臂和目标物体的工作台，尺寸为 $1.21*0.76*0.75m$

#### 可视化坐标轴
  用于可视化立方体需要被提升到的目标位置

### 任务目标

机器人需要完成以下操作流程：

1.  **接近物体**：末端执行器移动到立方体位置。
2.  **抓取物体**：控制夹爪闭合以抓住立方体。
3.  **提升与移动**：将立方体提升并移动到随机生成的目标坐标（Commands）。
4.  **稳定控制**：在移动过程中保持动作平滑，避免剧烈抖动。

-----

## 动作空间

动作空间为 `Box(-inf, inf, (8,), float32)`，前7维用于控制机械臂的关节运动，最后一维控制夹爪的开闭。

### 控制模式

环境采用混合控制模式：前7维为关节位置偏移控制，最后1维为夹爪的二元开闭控制（闭合/打开）。

**1. 机械臂关节 (Dim 0-6):**

```
目标关节角度 = 初始关节角度 + 动作值 (作为偏移量)
```

注意：最终的关节角度会被截断在 `control_config.min_pos` 和 `max_pos` 之间。

**2. 夹爪控制 (Dim 7):**
夹爪的二元开闭控制动作采用**伯努利概率采样**实现：

  - 输入值$action[7]$经过 Sigmoid 函数映射为概率 $p = 1 / (1 + e^{-action[7]})$。
  - 生成随机数 $r \sim U(0, 1)$。
  - 若 $r < p$，输出 0 (闭合夹爪)；否则输出 0.04 (打开夹爪)。



### 动作维度详细说明

策略输出的动作定义为各个关节的位置偏移量（Position Offset），其每一个维度对应机械臂的一个关节执行器（Actuator）。

底层仿真使用位置执行器（Position Actuator）进行驱动，具体的 PD 控制参数（KP 比例增益 / KV 微分增益）与物理限制定义在 MJCF 文件（ `mjx_panda.xml`）的 `<actuator>` 标签中。

`mjx_panda.xml`定义的Actuator物理限制与控制参数的详细列表：

| 编号 | 关节说明 | 控制范围 (CtrlRange) | 对应关节名称 | 关节类型 | KP (P增益) | KV (D增益) | 单位 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | 关节1旋转 (基座) | -2.8973 ~ 2.8973 | joint1 | hinge | 400 | 80 | rad |
| 1 | 关节2旋转 | -1.7628 ~ 1.7628 | joint2 | hinge | 400 | 80 | rad |
| 2 | 关节3旋转 | -2.8973 ~ 2.8973 | joint3 | hinge | 400 | 80 | rad |
| 3 | 关节4旋转 (肘部) | -3.0718 ~ -0.0698 | joint4 | hinge | 400 | 80 | rad |
| 4 | 关节5旋转 | -2.8973 ~ 2.8973 | joint5 | hinge | 400 | 80 | rad |
| 5 | 关节6旋转 | -0.0175 ~ 3.7525 | joint6 | hinge | 400 | 80 | rad |
| 6 | 关节7旋转 (腕部) | -2.8973 ~ 2.8973 | joint7 | hinge | 400 | 80 | rad |
| 7 | 夹爪开合控制 | 0 (闭) ~ 0.04 (开) | finger_joint1 | slide | 350 | - * | m |

> **注**：夹爪（Actuator 8）在配置文件中使用 `<general>` 执行器，其刚度（KP）由 `gainprm="350..."` 设定，未显式设定 Actuator 级的 KV，阻尼主要依赖关节自身的物理阻尼（`damping="10"`）。
  
-----

## 观测空间

观测空间为 `Box(-inf, inf, (36,), float32)`，包含机器人的关节状态、物体位姿、目标信息及历史动作。

### 观测组成部分

观测向量由以下部分组成（按顺序拼接）：

1.  **关节相对位置（9维）**：
      - 当前关节角度减去默认关节角度 (`default_joint_pos`)。包含7个臂关节和2个指关节。
2.  **关节相对速度（9维）**：
      - 当前关节速度减去初始速度（初始为0）。
3.  **物体位姿（7维）**：
      - 立方体在世界坐标系下的位置 (3维) 和 四元数 (4维)。
4.  **目标位置（3维）**：
      - 当前任务指定的目标位置的世界坐标系下位置 (x, y, z)。
5.  **上一时刻动作（8维）**：
      - 上一步执行的动作向量，其中 Dim 7 为使用原始输入数值而非二元开闭映射后的夹爪控制值。

### 观测详细说明

| 编号 | 观测内容 | 维度 | 说明 |
| --- | --- | --- | --- |
| 0-8 | 关节相对位置 | 9 | `dof_pos - default_pos` |
| 9-17 | 关节相对速度 | 9 | `dof_vel - init_vel` |
| 18-24 | 物体位姿 (Pos + Quat) | 7 | 立方体的世界坐标位姿 |
| 25-27 | 目标位置 (Command) | 3 | 立方体需要到达的目标点 |
| 28-35 | 上一时刻动作 | 8 | `info["current_actions"]` |

-----



## 奖励函数

奖励函数采用复合设计，旨在引导机器人完成接近、抓取、提升、移动至目标点以及最终的静止稳定。奖励权重包含固定权重与根据训练阶段动态调整的惩罚权重。

### 任务奖励项

1.  **接近奖励 (Reach Reward)**
    * **计算公式**：`1 - tanh(hand_cube_distance / 0.1)`
    * **目的**：激励末端执行器快速靠近立方体。
    * **权重**：1.0

2.  **提升奖励 (Lift Reward)**
    * **条件**：立方体高度 > 0.04m (4cm) 且 末端与物体距离 < 0.05m。
    * **目的**：鼓励机器人在抓稳物体的情况下将其提升离开桌面。
    * **权重**：10

3.  **目标进度奖励 (Command Progress Reward)**
    * **计算公式**：`1 - tanh(progress / 0.4)`
        * 其中 `progress = object_command_dist / max_length` (归一化距离)。
    * **条件**：仅在物体被提升（Lifted）状态下生效。
    * **目的**：引导机器人将物体从初始位置向目标位置搬运的整体进度。
    * **权重**：100

4.  **目标追踪奖励 (Command Tracking Reward)**
    * **计算公式**：`1 - tanh(object_command_dist / 0.3)`
    * **条件**：物体高度 > 0.04m 且 末端与物体距离 < 0.02m (抓握紧密)。
    * **目的**：在搬运过程中保持对目标的粗略追踪。
    * **权重**：20

5.  **目标到达奖励 (Command Reaching Reward)**
    * **计算公式**：`1 - tanh(object_command_dist / 0.05)`
    * **条件**：物体距离目标 < 0.3m 且 末端与物体距离 < 0.02m。
    * **目的**：在接近目标区域时提供高精度的位置引导（小带宽 0.05）。
    * **权重**：220

6.  **静止奖励 (End Still Reward)**
    * **计算公式**：`1 - tanh(joint_vel_sq / 0.4)`
    * **条件**：物体距离目标 < 0.04m (已到达目标)。
    * **目的**：鼓励机器人在完成任务后保持关节静止，减少抖动。
    * **权重**：50

### 惩罚项

* **动作变化惩罚 (Action Penalty)**：惩罚动作的剧烈变化 `sum((current_action - last_action)^2)`。
* **关节速度惩罚 (Joint Velocity Penalty)**：惩罚过大的关节速度 `sum(joint_vel^2)`。
* **关节位置偏移惩罚 (Joint DOF Penalty)**：惩罚关节偏离初始位置的程度 `sum((dof_pos - init_pos)^2)`，防止出现怪异姿态。权重固定为 `1e-2`。

**动态权重调整：**

* **前 10000 步**：
    * 动作变化惩罚系数：`1e-4`
    * 关节速度惩罚系数：`1e-4`
* **10000 步后**：
    * 动作变化惩罚系数增加至 `1e-1`
    * 关节速度惩罚系数增加至 `1e-1`
    * **目的**：在训练初期鼓励探索，后期强化平滑控制。

-----

## 初始状态

### 机器人初始化

**关节初始化：**

  - 机器人关节初始化为 `init_state.default_joint_pos`。
  - 施加随机噪声：在 `[-0.125, 0.125]` 范围内均匀采样添加到初始关节角度。

### 环境物体初始化

**立方体位置：**

  - 立方体在桌面上随机生成，位置范围：
      - X: [-0.1, 0.1]
      - Y: [-0.25, 0.25]
  - Z: 固定为 0.05m。

### 目标生成 (Commands)

目标位置在三维空间内随机采样，范围由 `cfg.command_config` 定义：

  - **Target X**: [0.4, 0.6]
  - **Target Y**: [-0.25, 0.25]
  - **Target Z**: [0.25, 0.5]。

-----

## 回合终止

### 终止条件 (Truncated)

环境在以下任一条件满足时终止回合（Truncated 为 True）：

1.  **超时终止**

      - 条件：回合步数超过 `max_episode_seconds` (2.5s) 对应的步数。

2.  **物体掉落**

      - 条件：立方体高度小于 -0.05m。

3.  **速度异常**

      - **关节速度**：任一关节速度绝对值超过 10 rad/s。
      - **物体速度**：立方体线速度绝对值超过 10 m/s。

### 成功判定

当立方体位置与目标位置重合时，应视为成功，但本实现中不结束该回合，立方体应保持当前位置不动，直到本回合结束。

-----

## 配置参数

Franka Lift Cube 环境通过 `FrankaLiftCubeEnvCfg` 类进行配置。

### 关键配置参数说明

| 参数类别 | 参数名称 | 默认值 | 说明 |
| --- | --- | --- | --- |
| **基础配置** | max\_episode\_seconds | 2.5 | 单回合最大时长 (秒) |
|  | sim\_dt | 0.01 | 仿真步长 |
|  | ctrl\_dt | 0.01 | 控制步长(回合长度=max\_episode\_seconds/sim\_dt) |
| **初始状态** | init\_state.joint\_pos\_reset\_noise | 0.125 | 机械臂关节角度初始随机噪声范围 |
|  | init\_state.cube_pos_x_reset_noise |[-0.1, 0.1] | 立方体X轴初始随机噪声范围 |
|  | init\_state.cube_pos_y_reset_noise | [-0.25, 0.25] | 立方体Y轴初始随机噪声范围 |
| **命令配置** | command\_config.target\_pos\_x | [0.4, 0.6] | 目标X轴生成范围 |
|  | command\_config.target\_pos\_y | [-0.25, 0.25] | 目标Y轴生成范围 |
|  | command\_config.target\_pos\_z | [0.25, 0.5] | 目标Z轴生成范围 |
-----

## 版本历史

### v1.0

**核心功能：**

  - 实现基于 Franka Panda 的 Pick-and-Lift 任务。
  - 引入概率性夹爪控制策略（Probabilistic Gripper Control）。
  - 设计了包含 Shifted Sigmoid 的分层奖励函数。
  - 支持并行环境仿真。
---

**已知限制：**

  - 仅支持机械臂的关节位置控制
  - 未实现动态目标跟踪（目标在回合中固定）

-----


## 使用示例

### 基本使用

模型训练示例
```python
uv run ./scripts/train.py --env franka_open_cabinet  --num-envs 4096  # --render 可选是否进行渲染
```

模型运行示例
```python
uv run ./scripts/play.py --env franka_open_cabinet  --num-envs 20
```

* 如果渲染时，环境之间的间距过小，可以修改 `MotrixLab\motrix_envs\src\motrix_envs\np\renderer.py`中`NpRenderer`的__init__方法的`spacing = 2.0`

-----


## 常见问题

### 1. 仿真崩溃 (Simulation Crash/NaN/Inf)

**可能原因：**

- **关节速度过大**：导致物理引擎计算发散，出现 `NaN` 或 `Inf`。
- **物块速度过大**：物块被击飞，速度超出物理限制。

**解决方案：**

- 设置关节速度阈值（例如限制 1 step 转动不超过 1/40 弧度）。
- 在送入电机前直接对 Action 数据进行 Clip（截断），防止动作值越界。



### 2. 渲染时机械臂运动抖动

**可能原因：**

- `sim_dt`（仿真步长）与 `ctrl_dt`（控制步长）配合不佳。仿真步长过大影响物理计算稳定性，控制步长过大会导致控制不够细腻。
- 位置控制器的 PD 参数设置不合理，特别是 D 参数（微分项）过小，导致阻尼不足。

**解决方案：**

- **统一时间步长**：建议将仿真参数 `sim_dt` 和 `ctrl_dt` 均设置为 `0.01` (100Hz)。
- **调整 PD 参数**：D 参数过小是导致抖动的重要原因。建议将xml模型中actuator的 D 参数调整为 P 参数开平方的 2 倍（即 $D = 2\sqrt{P}$），以达到临界阻尼状态，抑制震荡。
- **预期效果**：调整后机械臂动作会变得柔顺



### 3. 出现“击飞”式作弊 (Reward Hacking)

**现象：**
机械臂不抓取物体，而是直接用力撞击物块使其飞起以骗取高度奖励。

**可能原因：**
`lifting_object` 奖励函数设计不严谨，仅判断了 `cube_height > 0.2`，未限制机械臂必须“抓住”物体。

**解决方案：**
增加限制条件，在计算 Lift 奖励时，必须同时满足 `hand_cube_distance < threshold`（例如手与物块距离小于阈值）。


### 4. 机械臂夹取物块时出现过度改变关节位置(姿势诡异)

**可能原因：**
- 在训练过程中，学到了不好的姿势却能够成功夹取物块获取物块。

**解决方案：**

- **增加改变量惩罚项**：将当前关节位置值和初始位置值的差值的平方和作为惩罚项，加入到奖励函数中，惩罚机械臂过度改变关节位置。


### 5. 夹住物块后，机械臂难以前往指令位置

**现象：**
机械臂抓取物体后，停在某处或者向其它方向移动，而不是前往指令位置。

**可能原因：**
奖励函数设置不当，机械臂移动方块时获得的奖励过低，导致机械臂没有学习到前往指令位置。

**解决方案：**
如本项目，调整奖励函数，让机械臂缩小方块与指令位置的距离，从而获得更高的奖励。

### 6. 机械臂夹取物体到达目标位置后抖动

**可能原因：**

- 未设置到达目标位置后的静止奖励；
- 机械臂动作控制不合理（如将策略输出作为当前关节位置的偏移，让机械臂保持静止需要策略输出均趋于0，浅层神经网络学习较为困难）。

**解决方案：**

- **增加静止奖励**：在奖励函数中添加一个静止奖励项，当机械臂夹取物块到达目标位置后，各个关节速度接近于0时，给予额外奖励。
- **修改动作控制方式**：将当前策略的输出值作为初始关节位置的偏移量，而不是当前关节位置的偏移量。





-----



## 引用

如果您在研究中使用此环境，请引用：

```
@misc{motrixlab_anymalc_nav,
  title={ANYmal-C Navigation Task Environment},
  author={MotrixLab Contributors},
  year={2025},
  publisher={GitHub},
  url={https://github.com/your-repo/MotrixLab}
}
```


-----

## 许可证

本环境遵循 Apache License 2.0 开源协议。

---

## 相关资源

- **MotrixSim 文档**：[链接待补充]
- **ANYmal-C 机器人官方资料**：https://www.anybotics.com/
- **Gymnasium 文档**：https://gymnasium.farama.org/
- **训练示例脚本**：`scripts/train.py`
- **可视化脚本**：`scripts/view.py`